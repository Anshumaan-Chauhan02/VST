{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZpkjX988N27m",
        "2LFHqBv2N9SP",
        "lP85NPxfN_7d"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing BookNLP"
      ],
      "metadata": {
        "id": "ZpkjX988N27m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LcktL1ItHUq",
        "outputId": "d1e418b6-e121-4be7-d11b-a75780ae2834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting booknlp\n",
            "  Downloading booknlp-1.0.7.tar.gz (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from booknlp) (2.0.1+cu118)\n",
            "Requirement already satisfied: tensorflow>=1.15 in /usr/local/lib/python3.10/dist-packages (from booknlp) (2.12.0)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.10/dist-packages (from booknlp) (3.5.2)\n",
            "Collecting transformers>=4.11.3 (from booknlp)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->booknlp) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.15->booknlp) (0.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->booknlp) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->booknlp) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->booknlp) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->booknlp) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.1->booknlp) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.1->booknlp) (16.0.5)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.11.3->booknlp)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->booknlp) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->booknlp) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.11.3->booknlp)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.15->booknlp) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.11.3->booknlp) (2023.4.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.15->booknlp) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.15->booknlp) (1.10.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->booknlp) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->booknlp) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->booknlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->booknlp) (3.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (2.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3->booknlp) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3->booknlp) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3->booknlp) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3->booknlp) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->booknlp) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.15->booknlp) (3.2.2)\n",
            "Building wheels for collected packages: booknlp\n",
            "  Building wheel for booknlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for booknlp: filename=booknlp-1.0.7-py3-none-any.whl size=2420743 sha256=098040d13c78927b41b7adbb98bd559935354e8c58d75a47c56a10214bcefc1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/f5/b4/2ee6b317da6491ff01b742be603af81ceea156427f1f4be38a\n",
            "Successfully built booknlp\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, booknlp\n",
            "Successfully installed booknlp-1.0.7 huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install booknlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the datasets"
      ],
      "metadata": {
        "id": "2LFHqBv2N9SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "df_movie = pd.read_csv('/content/drive/MyDrive/Visual Story Telling/Dataset - Story Generation/CMU_Movie_Dataset')\n",
        "df_books = pd.read_csv('/content/drive/MyDrive/Visual Story Telling/Dataset - Story Generation/CMU_Book_Dataset')"
      ],
      "metadata": {
        "id": "aTl4rv0Vt0Js"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_movie['Characters'] = \"\""
      ],
      "metadata": {
        "id": "T4CXWQbFp_Jf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_movie = df_movie.assign(Relations=\"\")\n",
        "df_books = df_books.assign(Characters=\"\",Relations=\"\")"
      ],
      "metadata": {
        "id": "qTvx2kf_1mvH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_movie"
      ],
      "metadata": {
        "id": "8mwyLVD0pcOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_books"
      ],
      "metadata": {
        "id": "JWMGXEkWpd7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BookNLP Implementation + Sentiment Extraction\n",
        "\n",
        "1) Implementing BookNLP on the datasets \\\\\n",
        "2) Extraction of sentences with two people in it connected using a verb \\\\\n",
        "3) Getting the sentiment using NLTK Sentiment Analyzer "
      ],
      "metadata": {
        "id": "lP85NPxfN_7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the pipeline - Installing the Models necessary for different processing done by BookNLP\n",
        "from booknlp.booknlp import BookNLP\n",
        "\n",
        "model_params={\n",
        "\t\t\"pipeline\":\"entity,quote,supersense,event,coref\", \n",
        "\t\t\"model\":\"big\"\n",
        "\t}\n",
        "\t\n",
        "booknlp=BookNLP(\"en\", model_params)"
      ],
      "metadata": {
        "id": "x7fr61aCtcCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the NLTK Sentiment Analyzer VADER\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTWT8m159AKM",
        "outputId": "d2388763-5133-4840-d1b8-6d5d41403d63"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_booknlp_movies(x):\n",
        "  # Creating file for the summary \n",
        "  unique_id = x['Wikipedia movie ID']\n",
        "  \n",
        "  with open(f'{unique_id}.txt', 'w') as f:\n",
        "    f.write(x['Summary'])\n",
        "  \n",
        "  # Input file to process\n",
        "  input_file = f'{unique_id}.txt'\n",
        "\n",
        "  # Output directory to store resulting files in\n",
        "  output_directory = \"./create_dataset/movie/\"\n",
        "\n",
        "  # File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
        "  book_id = x['Wikipedia movie ID']\n",
        "\n",
        "  # Process the file \n",
        "  booknlp.process(input_file, output_directory, book_id)\n",
        "\n",
        "  # Loading the .tokens file in a table format for extraction of sentences\n",
        "  df_sentences = pd.read_table(f'/content/create_dataset/movie/{book_id}.tokens')\n",
        "  sentences = df_sentences.groupby(['sentence_ID'])['word'].apply(lambda x: ' '.join(x)).values\n",
        "\n",
        "  sentences = sentences.tolist()\n",
        "\n",
        "  lengths = [len(x.split()) for x in sentences]\n",
        "\n",
        "  # Calculating the cumulative lengths of the sentences \n",
        "  for i in range(len(lengths)):\n",
        "    if i!=0:\n",
        "      lengths[i] = lengths[i]+lengths[i-1]\n",
        "  \n",
        "  # .entities file consists of different components such as category, start token, etc that will be used for extraction \n",
        "  df = pd.read_table(f'/content/create_dataset/movie/{book_id}.entities')\n",
        "  \n",
        "  text_values = df.text.values\n",
        "  ref_id = df.COREF.values \n",
        "  cat = df.cat.values\n",
        "  start_token_id = df.start_token.values\n",
        "  type_of_identity = df.prop.values\n",
        "\n",
        "  # .verb file will be utilized to filter sentences where the identities are connected with an verb\n",
        "  df_verb = pd.read_table(f'/content/create_dataset/movie/{book_id}.supersense')\n",
        "\n",
        "  # Dictionary to hold identity paris along with the sentences in which they were referenced\n",
        "  paired_sent = {}\n",
        "\n",
        "  # Extraction of sentences with two identities connected using a verb\n",
        "  for j in range(len(sentences)):\n",
        "    check = False\n",
        "    identities = []\n",
        "    for i in range(len(text_values)):\n",
        "      # Do not want to check for identities that are referenced ebfore or after the current sentence\n",
        "      if start_token_id[i]<lengths[j-1] and i!=0:\n",
        "        continue\n",
        "      if start_token_id[i]>lengths[j]:\n",
        "        break\n",
        "      # Identities should only be PER (person) and referred as a PROP (Proper Noun)\n",
        "      if text_values[i] in sentences[j] and cat[i]=='PER' and type_of_identity[i]==\"PROP\":\n",
        "        identities.append([text_values[i], ref_id[i]])\n",
        "    if len(identities)==2 and identities[1][0]!=identities[0][0]:\n",
        "      check = True\n",
        "    for idx, row in df_verb.iterrows():\n",
        "      if \"verb.social\" in row.supersense_category and check:\n",
        "        if row.start_token>lengths[j]:\n",
        "          break  \n",
        "        if row.start_token<lengths[j-1] and i!=0:\n",
        "          continue\n",
        "        # Collection of all sentences for the pair extracted earlier\n",
        "        if f'{identities[0][0]} and {identities[1][0]}' in paired_sent or f'{identities[1][0]} and {identities[0][0]}' in paired_sent:\n",
        "          if paired_sent.get(f'{identities[0][0]} and {identities[1][0]}')!=None:\n",
        "            paired_sent[f'{identities[0][0]} and {identities[1][0]}'] += \" \"+sentences[j]\n",
        "            break\n",
        "          else:\n",
        "            paired_sent[f'{identities[1][0]} and {identities[0][0]}'] += \" \"+sentences[j]\n",
        "            break\n",
        "        else:\n",
        "          paired_sent[f'{identities[0][0]} and {identities[1][0]}'] = \"\"\n",
        "          paired_sent[f'{identities[0][0]} and {identities[1][0]}'] += \" \"+sentences[j]\n",
        "          break\n",
        "  \n",
        "  # Extracting character names \n",
        "  with open(f'/content/create_dataset/movie/{book_id}.book.html', 'r') as f:\n",
        "    contents = f.read()\n",
        "  doc = BeautifulSoup(contents, \"html.parser\")\n",
        "  characters = []\n",
        "  tag = doc.findAll(True)[0] \n",
        "  for idx,i_tag in enumerate(tag):\n",
        "    if idx>0 and idx%2==0 and idx<len(tag)-1:\n",
        "      char_name = ' '.join(i_tag.split(\"/\")[0].split()[1:-1])\n",
        "      if char_name!=\"\":\n",
        "        characters.append(char_name)\n",
        "\n",
        "  # Getting a sentiment for the pairs, according to the sentences extracted\n",
        "  relations = {'neu':\"neutral\",'pos':\"positive\",'neg':\"negative\"}\n",
        "  inter_char_relations = []\n",
        "  for key in paired_sent:\n",
        "    ss = SentimentIntensityAnalyzer().polarity_scores(paired_sent[key])\n",
        "    del ss['compound']\n",
        "    inter_char_relations.append(f'{key} have {relations[max(ss, key=ss.get)]} relationship')\n",
        "\n",
        "  return (\", \".join(characters),\". \".join(inter_char_relations))"
      ],
      "metadata": {
        "id": "E_IDuBz7oinX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 20000\n",
        "df_movie = df_movie[:num_samples]"
      ],
      "metadata": {
        "id": "rWZLbm-C8jO4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, row in df_movie.iterrows():\n",
        "  df_movie['Characters'].iloc[idx], df_movie['Relations'].iloc[idx] =  apply_booknlp_movies(row)"
      ],
      "metadata": {
        "id": "wvEBG1xv9QMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_booknlp_books(x):\n",
        "\n",
        "  # Creating file for the summary \n",
        "  unique_id = x['Wikipedia article ID']\n",
        "  \n",
        "  with open(f'{unique_id}.txt', 'w') as f:\n",
        "    f.write(x['Plot summary'])\n",
        "  \n",
        "  # Input file to process\n",
        "  input_file = f'{unique_id}.txt'\n",
        "\n",
        "  # Output directory to store resulting files in\n",
        "  output_directory = \"./create_dataset/book/\"\n",
        "\n",
        "  # File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
        "  book_id = x['Wikipedia article ID']\n",
        "\n",
        "  # Process the file \n",
        "  booknlp.process(input_file, output_directory, book_id)\n",
        "\n",
        "  # Loading the .tokens file in a table format for extraction of sentences\n",
        "  df_sentences = pd.read_table(f'/content/create_dataset/book/{book_id}.tokens')\n",
        "  sentences = df_sentences.groupby(['sentence_ID'])['word'].apply(lambda x: ' '.join(x)).values\n",
        "\n",
        "  sentences = sentences.tolist()\n",
        "\n",
        "  lengths = [len(x.split()) for x in sentences]\n",
        "\n",
        "  # Calculating the cumulative lengths of the sentences \n",
        "  for i in range(len(lengths)):\n",
        "    if i!=0:\n",
        "      lengths[i] = lengths[i]+lengths[i-1]\n",
        "  \n",
        "  # .entities file consists of different components such as category, start token, etc that will be used for extraction \n",
        "  df = pd.read_table(f'/content/create_dataset/book/{book_id}.entities')\n",
        "  \n",
        "  text_values = df.text.values\n",
        "  ref_id = df.COREF.values \n",
        "  cat = df.cat.values\n",
        "  start_token_id = df.start_token.values\n",
        "  type_of_identity = df.prop.values\n",
        "\n",
        "  # .verb file will be utilized to filter sentences where the identities are connected with an verb\n",
        "  df_verb = pd.read_table(f'/content/create_dataset/book/{book_id}.supersense')\n",
        "\n",
        "  # Dictionary to hold identity paris along with the sentences in which they were referenced\n",
        "  paired_sent = {}\n",
        "\n",
        "  # Extraction of sentences with two identities connected using a verb\n",
        "  for j in range(len(sentences)):\n",
        "    check = False\n",
        "    identities = []\n",
        "    for i in range(len(text_values)):\n",
        "      # Do not want to check for identities that are referenced ebfore or after the current sentence\n",
        "      if start_token_id[i]<lengths[j-1] and i!=0:\n",
        "        continue\n",
        "      if start_token_id[i]>lengths[j]:\n",
        "        break\n",
        "      # Identities should only be PER (person) and referred as a PROP (Proper Noun)\n",
        "      if text_values[i] in sentences[j] and cat[i]=='PER' and type_of_identity[i]==\"PROP\":\n",
        "        identities.append([text_values[i], ref_id[i]])\n",
        "    if len(identities)==2 and identities[1][0]!=identities[0][0]:\n",
        "      check = True\n",
        "    for idx, row in df_verb.iterrows():\n",
        "      if \"verb.social\" in row.supersense_category and check:\n",
        "        if row.start_token>lengths[j]:\n",
        "          break  \n",
        "        if row.start_token<lengths[j-1] and i!=0:\n",
        "          continue\n",
        "        # Collection of all sentences for the pair extracted earlier\n",
        "        if f'{identities[0][0]} and {identities[1][0]}' in paired_sent or f'{identities[1][0]} and {identities[0][0]}' in paired_sent:\n",
        "          if paired_sent.get(f'{identities[0][0]} and {identities[1][0]}')!=None:\n",
        "            paired_sent[f'{identities[0][0]} and {identities[1][0]}'] += \" \"+sentences[j]\n",
        "            break\n",
        "          else:\n",
        "            paired_sent[f'{identities[1][0]} and {identities[0][0]}'] += \" \"+sentences[j]\n",
        "            break\n",
        "        else:\n",
        "          paired_sent[f'{identities[0][0]} and {identities[1][0]}'] = \"\"\n",
        "          paired_sent[f'{identities[0][0]} and {identities[1][0]}'] += \" \"+sentences[j]\n",
        "          break\n",
        "  \n",
        "  # Extracting character names \n",
        "  with open(f'/content/create_dataset/book/{book_id}.book.html', 'r') as f:\n",
        "    contents = f.read()\n",
        "  doc = BeautifulSoup(contents, \"html.parser\")\n",
        "  characters = []\n",
        "  tag = doc.findAll(True)[0] \n",
        "  for idx,i_tag in enumerate(tag):\n",
        "    if idx>0 and idx%2==0 and idx<len(tag)-1:\n",
        "      char_name = ' '.join(i_tag.split(\"/\")[0].split()[1:-1])\n",
        "      if char_name!=\"\":\n",
        "        characters.append(char_name)\n",
        "\n",
        "  # Getting a sentiment for the pairs, according to the sentences extracted\n",
        "  relations = {'neu':\"neutral\",'pos':\"positive\",'neg':\"negative\"}\n",
        "  inter_char_relations = []\n",
        "  for key in paired_sent:\n",
        "    ss = SentimentIntensityAnalyzer().polarity_scores(paired_sent[key])\n",
        "    del ss['compound']\n",
        "    inter_char_relations.append(f'{key} have {relations[max(ss, key=ss.get)]} relationship')\n",
        "\n",
        "  return (\", \".join(characters),\". \".join(inter_char_relations))"
      ],
      "metadata": {
        "id": "YFTWmH5apqvS"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 10\n",
        "df_books = df_books[:num_samples]"
      ],
      "metadata": {
        "id": "bg1IBit67jX-"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, row in df_books.iterrows():\n",
        "  df_books['Characters'].iloc[idx], df_books['Relations'].iloc[idx] =  apply_booknlp_books(row)"
      ],
      "metadata": {
        "id": "5MeDNreR_A9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Concatenate the datasets\n",
        "final_df = pd.concat([])"
      ],
      "metadata": {
        "id": "Tl5YHlEFAull"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the dataset\n",
        "final_df.to_csv('/content/drive/MyDrive/Visual Story Telling/Dataset - Story Generation/Training_Dataset', index=False)"
      ],
      "metadata": {
        "id": "eecXBAsuAcOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
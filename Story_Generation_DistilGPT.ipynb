{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9890760d",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c61953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshj\\anaconda3\\envs\\model_train\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from tqdm import tqdm, trange\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cdebab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"]=\"1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1535be5",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1ff9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0ea34",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917eeeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(x, base_tokenizer=base_tokenizer):\n",
    "    return x[\"Input\"]+\" \"+\"Summary: \"+x['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a3bf89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Plot_Summary_Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05eeaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Input'] = df.apply(combine, axis=1)\n",
    "df = df['Input']\n",
    "df.to_csv('Plot_Summary_DistilGPT2', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073b76c",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51dc2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"Plot_Summary_DistilGPT2\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83127464",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('P_S_D_Train',index=False)\n",
    "test_df.to_csv('P_S_D_Test',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3c5ce",
   "metadata": {},
   "source": [
    "## Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf88d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(x, base_tokenizer = base_tokenizer):\n",
    "    # Max length of the input sequence in DistilGPT2 is 1024 tokens \n",
    "    return base_tokenizer(x, max_length = 1024, truncation=True, add_special_tokens = True)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee77cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"P_S_D_Train\")\n",
    "test_df = pd.read_csv(\"P_S_D_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f133f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df['Input'].apply(encode_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d72dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df['Input'].apply(encode_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e6b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace = True)\n",
    "test_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4d1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e8925",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13e43bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting pad token same as eos token\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "#Initializing Data Collator that forms batches and sends the input in a proper language modeling format to the model for training and evaluation\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=base_tokenizer,\n",
    "        mlm=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5cb5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Story_Gen_Model/DistilGPT2'\n",
    "\n",
    "#Specifying the Training arguments for the model training \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    overwrite_output_dir = True, \n",
    "    evaluation_strategy=\"no\", \n",
    "    gradient_accumulation_steps=8, \n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4, \n",
    "    fp16=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d319c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_df,       # training dataset\n",
    "    eval_dataset = test_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af75007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc9740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resuming training from a checkpoint trained with 4.27.2 of Transformers but your current version is 4.29.2. This is not recommended and could yield to errors or unwanted behaviors.\n",
      "C:\\Users\\anshj\\anaconda3\\envs\\model_train\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16290' max='16290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16290/16290 15:44:40, Epoch 29/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.992300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.978400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.951400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.945400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16290, training_loss=1.0405259387049666, metrics={'train_runtime': 56691.3036, 'train_samples_per_second': 18.392, 'train_steps_per_second': 0.287, 'total_flos': 1.5087687350530867e+17, 'train_loss': 1.0405259387049666, 'epoch': 30.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting the training\n",
    "trainer.train(resume_from_checkpoint=\"Story_Gen_Model/DistilGPT2/checkpoint-8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92f0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f29a2d",
   "metadata": {},
   "source": [
    "## Model Evaluation (Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a91d242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='483' max='483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [483/483 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "871d63c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.0107593536376953,\n",
       " 'eval_runtime': 18.2229,\n",
       " 'eval_samples_per_second': 211.931,\n",
       " 'eval_steps_per_second': 26.505}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30e3c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 20.30\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81467f",
   "metadata": {},
   "source": [
    "## Loading and Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31e56cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"Story_Gen_Model/DistilGPT2\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cab4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer.pad_token = base_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc37758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Summary: Goku is a god of darkness, destroying everything in his way. He's been raised by an alien called a Gowan. When an alien called Gowan crashes into Goku's village, Goku and his new family are forced to escape from the Gowan and into the desert, but Goku survives by transforming into a powerful being. However, it is gone for a time and it's time to warn the world of a Bulma God of the approaching disaster. Goku and Vegeta are now in the service of Bulma King, who has ordered the destruction of the entire planet. In order to protect their family Goku and Vegeta are forced to protect their village from Bulma King's evil sorceress. However, unbeknownst to them, Gowan plans to use the Gowan's gravity to transform Goku into Uranus. When they\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Format of the input\n",
    "Title: ADD_TITLE. Genre: ADD_GENRES. Characters: ADD_CHARS. Relations: Neutral: <>. Positive: <>. Negative: <>.  Plot: ADD_PLOT\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "input = \"Title: Saiyan Warrior. Genre: Action, Adventure, Drama. Characters: Goku, Vegeta, Beerus. Relations: Neutral: <Beerus, Vegeta>. Positive: <Goku, Vegeta>. Negative: <Goku, Beerus>. Plot: Beerus is a god of destruction, who came to Earth to destroy it. Goku and Vegeta in order to protect their family fight and defeat him.\"\n",
    "generator = pipeline(\"text-generation\", model=base_model, tokenizer = base_tokenizer)\n",
    "do_sample = True\n",
    "num_beams = 1\n",
    "top_p = 0.9\n",
    "generator(input, num_beams = num_beams, top_p = top_p, do_sample = do_sample, min_length =150, max_length = 250)[0]['generated_text'][len(input)+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2377e48",
   "metadata": {},
   "source": [
    "## BLEU Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df212cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "333baf8e",
   "metadata": {},
   "source": [
    "## Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa92f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9dceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('Add path to the json file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041aeac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch(x):\n",
    "  return x['epoch']\n",
    "\n",
    "def get_loss(x):\n",
    "  return x['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['epoch_number'] = df['log_history'].apply(get_epoch)\n",
    "df ['loss'] = df['log_history'].apply(get_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['epoch_number'].values, df['loss'].values) \n",
    "plt.xlabel('#Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title('Training curve T5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-train",
   "language": "python",
   "name": "model-train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

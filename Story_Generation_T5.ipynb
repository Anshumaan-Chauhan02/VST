{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Transformers library"
      ],
      "metadata": {
        "id": "gMPsfSIMQTFs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vpJtgoRkQf8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3def58-850d-420f-b16d-fe0e2e52466c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "#Installing transformer package \n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rating Estimation by ChatGPT"
      ],
      "metadata": {
        "id": "pwf9LGiCzYmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "dzwU4eGtvhjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import openai\n",
        "os.environ[\"OpenAI_API_Key\"] = \"\"\n",
        "api_key = os.environ.get(\"OpenAI_API_Key\")\n",
        "openai.api_key = api_key"
      ],
      "metadata": {
        "id": "s67L7e5yvjSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss to be added in the custom loss\n",
        "def get_chatgpt_rating(prompt, sample):\n",
        "  completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \n",
        "     \"content\": f\"{prompt} {sample}\"}\n",
        "  ])\n",
        "  return 10 - int(completion.choices[0].message)"
      ],
      "metadata": {
        "id": "8tB5L6nFLhDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Different prompts tested in order to generate a sensible rating \n",
        "prompt1 = \"Provide me rating between 0 and 10 (without any explanation), where 0 is the best and 10 is the worst, for the following story summary: \" \n",
        "prompt2 = \"Assign a rating between 0 (best) and 10 (worst) to the given artificial story summary (only give rating as the response):\"\n",
        "prompt3 = \"Assign a rating between 0 (best) and 10 (worst) to the given artificial story summary (only give rating as the response). The rating should be based on writing style, coherence and capture strength. Summary:\"\n",
        "prompt4 = \"Assign a rating between 0 and 10 to the given artificial story summary. Only give rating as the response (no reasoning). The rating should be based on writing style, coherence, and capture strength. Summary:\" # Best\n",
        "prompt5 = \"Provide me rating between 0 and 10 (without any explanation),  for the following story summary:\" "
      ],
      "metadata": {
        "id": "MQJRNPghzb7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Tokenization/Encoding"
      ],
      "metadata": {
        "id": "2dYlSUZNQYop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the standard T5 small model and tokenizer \n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# Can try different T5 models such as T5-large, T5-3B, T5-11B\n",
        "base_tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "bVkVGdHBYQat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7a75d6-876d-4572-a22c-941de474b945"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the sequences\n",
        "def encode_sequences(x, base_tokenizer = base_tokenizer):\n",
        "  try:\n",
        "    # Input consists of different aspects of the story on which the output will be conditioned\n",
        "    input = str(x['Input']) \n",
        "    # Label is the conditioned output - Story\n",
        "    label = str(x['Summary'])\n",
        "    # Max length of the input sequence in T5 is 512 tokens (BART could be used for longer sequences - 1024 max length limit) \n",
        "    model_input = base_tokenizer(input, max_length = 512, truncation=True, padding='max_length')\n",
        "    model_input['labels'] = base_tokenizer(label, max_length = 512, truncation=True, padding='max_length')['input_ids']\n",
        "    return model_input\n",
        "  except:\n",
        "    # By performing this model will also be robust to empty inputs (a type of adversarial input)\n",
        "    input = ''\n",
        "    label = ''\n",
        "    model_input = base_tokenizer(input, max_length = 512, truncation=True, padding='max_length')\n",
        "    model_input['labels'] = base_tokenizer(label, max_length = 512, truncation=True, padding='max_length')['input_ids']\n",
        "    return model_input"
      ],
      "metadata": {
        "id": "pz4X3H2rMDTE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the final dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Visual Story Telling/Dataset - Story Generation/Movies/M_Dataset_0_2K')"
      ],
      "metadata": {
        "id": "9F_s4EA1YTER"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.1)"
      ],
      "metadata": {
        "id": "q7Y0PUMjTqO_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "fZr3FDXBoWgk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the dataset\n",
        "train_df = train_df.apply(encode_sequences, axis=1)\n",
        "test_df = test_df.apply(encode_sequences, axis = 1)"
      ],
      "metadata": {
        "id": "DTsB1rXyMp-x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5 Training Setup - Custom Loss Function"
      ],
      "metadata": {
        "id": "_ASa0fbnQe4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Data Collator for batching of the dataset\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=base_tokenizer,\n",
        "        return_tensors=\"pt\"\n",
        "    )"
      ],
      "metadata": {
        "id": "k_oMtcLHNMyo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# Path where model training loss and intermediate weights will be stored\n",
        "model_path = f'/content/drive/MyDrive/Visual Story Telling/Story_Gen_Model'\n",
        "\n",
        "#Specifying the training argument \n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=model_path,\n",
        "    per_device_train_batch_size=4, \n",
        "    overwrite_output_dir = True, \n",
        "    evaluation_strategy=\"no\", \n",
        "    gradient_accumulation_steps=8, \n",
        "    num_train_epochs=15,\n",
        "    weight_decay=0.01, \n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4 \n",
        "    fp16=True \n",
        ")"
      ],
      "metadata": {
        "id": "95wIk1YxNOTb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overwrite the Trainer API for utilizing custom loss function \n",
        "import torch.nn as nn\n",
        "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
        "    def compute_loss(self, model, inputs, tokenizer, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = custom_loss_function(logits, labels, tokenizer) \n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Check whether the tokenizer is being passed as an argument \n",
        "def custom_loss_function(logits, labels, tokenizer):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    loss_cross_entropy = loss_fct(logits,labels)\n",
        "    generated_summary = tokenizer.decode(logits)\n",
        "    loss_GPT = get_chatgpt_rating(prompt4, generated_summary)\n",
        "    return loss_cross_entropy+loss_GPT\n"
      ],
      "metadata": {
        "id": "iSFkn4YDOXl1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the trainer\n",
        "trainer = CustomSeq2SeqTrainer(\n",
        "    model=base_model,                         # the instantiated  Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_df,       # training dataset\n",
        "    eval_dataset = test_df,\n",
        ")"
      ],
      "metadata": {
        "id": "DQsfkoziNn2O"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training "
      ],
      "metadata": {
        "id": "gpPf0iO7QiYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting the training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RXKGeFzINqA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef21d6b-70b1-47b5-a6d2-7aa4b8c49df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the final model\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "u4ugp2eQNs_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "ZsCbDbfbwSBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.eval()"
      ],
      "metadata": {
        "id": "AM3GvSURwP3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to get BLEU Score rating of the model on test dataset \n",
        "\n",
        "\"\"\"\n",
        "Refer:\n",
        "\n",
        "1) https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n",
        "2) https://huggingface.co/docs/evaluate/choosing_a_metric\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "akIV58mywUES"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "gMPsfSIMQTFs",
        "pwf9LGiCzYmo",
        "2dYlSUZNQYop",
        "_ASa0fbnQe4u",
        "gpPf0iO7QiYn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Transformers library"
      ],
      "metadata": {
        "id": "gMPsfSIMQTFs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vpJtgoRkQf8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58469d09-60c0-4d86-d781-9feec592b652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "#Installing transformer package \n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rating Estimation by ChatGPT"
      ],
      "metadata": {
        "id": "pwf9LGiCzYmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "dzwU4eGtvhjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import openai\n",
        "os.environ[\"OpenAI_API_Key\"] = \"\"\n",
        "api_key = os.environ.get(\"OpenAI_API_Key\")\n",
        "openai.api_key = api_key"
      ],
      "metadata": {
        "id": "s67L7e5yvjSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss to be added in the custom loss\n",
        "def get_chatgpt_rating(prompt, sample):\n",
        "  completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \n",
        "     \"content\": f\"{prompt} {sample}\"}\n",
        "  ])\n",
        "  return 10 - int(completion.choices[0].message)"
      ],
      "metadata": {
        "id": "8tB5L6nFLhDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Different prompts tested in order to generate a sensible rating \n",
        "prompt1 = \"Provide me rating between 0 and 10 (without any explanation), where 0 is the best and 10 is the worst, for the following story summary: \" \n",
        "prompt2 = \"Assign a rating between 0 (best) and 10 (worst) to the given artificial story summary (only give rating as the response):\"\n",
        "prompt3 = \"Assign a rating between 0 (best) and 10 (worst) to the given artificial story summary (only give rating as the response). The rating should be based on writing style, coherence and capture strength. Summary:\"\n",
        "prompt4 = \"Assign a rating between 0 and 10 to the given artificial story summary. Only give rating as the response (no reasoning). The rating should be based on writing style, coherence, and capture strength. Summary:\" # Best\n",
        "prompt5 = \"Provide me rating between 0 and 10 (without any explanation),  for the following story summary:\" "
      ],
      "metadata": {
        "id": "MQJRNPghzb7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Tokenization/Encoding"
      ],
      "metadata": {
        "id": "2dYlSUZNQYop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the standard T5 small model and tokenizer \n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# Can try different T5 models such as T5-large, T5-3B, T5-11B\n",
        "base_tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "bVkVGdHBYQat"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the sequences\n",
        "def encode_sequences(x, base_tokenizer = base_tokenizer):\n",
        "  # try:\n",
        "    # Input consists of different aspects of the story on which the output will be conditioned\n",
        "    input = x['Input']\n",
        "    # Label is the conditioned output - Story\n",
        "    label = x['Summary']\n",
        "    # Max length of the input sequence in T5 is 512 tokens (BART could be used for longer sequences - 1024 max length limit) \n",
        "    model_input = base_tokenizer(input, max_length = 512, truncation=True, padding='max_length')\n",
        "    model_input['labels'] = base_tokenizer(label, max_length = 512, truncation=True, padding='max_length')['input_ids']\n",
        "    return model_input"
      ],
      "metadata": {
        "id": "pz4X3H2rMDTE"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode_sequences(train_df.iloc[0])"
      ],
      "metadata": {
        "id": "K92QLxdwq3Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the final dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Visual Story Telling/Dataset - Story Generation/Training_Dataset')"
      ],
      "metadata": {
        "id": "9F_s4EA1YTER"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.3)"
      ],
      "metadata": {
        "id": "q7Y0PUMjTqO_"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the dataset\n",
        "train_df = train_df.apply(encode_sequences, axis=1)\n",
        "test_df = test_df.apply(encode_sequences, axis = 1)"
      ],
      "metadata": {
        "id": "DTsB1rXyMp-x"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.dropna(inplace = True)\n",
        "test_df.dropna(inplace = True)"
      ],
      "metadata": {
        "id": "zqw4aSD8Agme"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "fZr3FDXBoWgk"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5 Training Setup (Includes Custom Loss Function)"
      ],
      "metadata": {
        "id": "_ASa0fbnQe4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Data Collator for batching of the dataset\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=base_tokenizer,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length = 512,\n",
        "        padding = 'max_length',\n",
        "        model = base_model\n",
        "    )"
      ],
      "metadata": {
        "id": "k_oMtcLHNMyo"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "jXxRUyW39Fr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# Path where model training loss and intermediate weights will be stored\n",
        "model_path = f'/content/drive/MyDrive/Visual Story Telling/Story_Gen_Model'\n",
        "\n",
        "#Specifying the training argument \n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=model_path,\n",
        "    per_device_train_batch_size=2, \n",
        "    overwrite_output_dir = True, \n",
        "    evaluation_strategy=\"no\", \n",
        "    gradient_accumulation_steps=8, \n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01, \n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4, \n",
        "    fp16=True \n",
        ")"
      ],
      "metadata": {
        "id": "95wIk1YxNOTb"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overwrite the Trainer API for utilizing custom loss function \n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "      labels = inputs.pop(\"labels\")\n",
        "      input_ids = inputs.pop(\"input_ids\")\n",
        "      attention_mask = inputs.pop(\"attention_mask\")\n",
        "      outputs = model(input_ids = input_ids, labels = labels, attention_mask = attention_mask)\n",
        "      loss_cross_entropy = outputs.loss\n",
        "      logits = outputs.logits\n",
        "      loss = custom_loss_function(logits) \n",
        "      return (loss_cross_entropy + torch.tensor(loss)).item()\n",
        "\n",
        "# Check whether the tokenizer is being passed as an argument \n",
        "def custom_loss_function(logits, tokenizer=base_tokenizer):\n",
        "    loss_GPT = 0\n",
        "    summaries = base_tokenizer.batch_decode(F.softmax(logits, dim=-1).argmax(dim=-1), skip_special_tokens=True)\n",
        "    for summary in summaries:\n",
        "      loss_GPT = loss_GPT + (10 - int(get_chatgpt_rating(prompt4, summary)))\n",
        "    return loss_GPT\n"
      ],
      "metadata": {
        "id": "iSFkn4YDOXl1"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the trainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=base_model,                         # the instantiated  Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_df,       # training dataset\n",
        "    eval_dataset = test_df,\n",
        ")"
      ],
      "metadata": {
        "id": "DQsfkoziNn2O"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Trainer function using Customized Loss function \n",
        "\n",
        "trainer = CustomSeq2SeqTrainer(\n",
        "    model=base_model,                         # the instantiated  Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_df,       # training dataset\n",
        "    eval_dataset = test_df,\n",
        ")"
      ],
      "metadata": {
        "id": "Mhswv40DCJef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training "
      ],
      "metadata": {
        "id": "gpPf0iO7QiYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting the training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RXKGeFzINqA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the final model\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "u4ugp2eQNs_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation (Perplexity)"
      ],
      "metadata": {
        "id": "ZsCbDbfbwSBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "AM3GvSURwP3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Eval (BLEU)"
      ],
      "metadata": {
        "id": "ooAGQEE-EBn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "oQ5o39Y0DcXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "#Loading the BLEU score metric\n",
        "bleu = load(\"bleu\")"
      ],
      "metadata": {
        "id": "hGayRQ7_Dind"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to get BLEU Score rating of the model on test dataset \n",
        "\n",
        "\"\"\"\n",
        "Refer:\n",
        "\n",
        "1) https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n",
        "2) https://huggingface.co/docs/evaluate/choosing_a_metric\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "akIV58mywUES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the samples\n",
        "# Tokenize them\n",
        "# Send to the model \n",
        "# Create pipeline or use generate function with different decoding algos\n",
        "# Store the gold label and the generated ones in a list \n",
        "# Get the bleu score with n-gram similarities"
      ],
      "metadata": {
        "id": "jW9xMTKLEGtq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
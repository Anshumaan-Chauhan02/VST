{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["a6xkj-aJUf6T"],"authorship_tag":"ABX9TyNhnsWQ6qaGPZXvTsibWumO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Installing Transformers library"],"metadata":{"id":"gMPsfSIMQTFs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpJtgoRkQf8x"},"outputs":[],"source":["#Installing transformer package \n","!pip install transformers"]},{"cell_type":"markdown","source":["## Rating Estimation by ChatGPT"],"metadata":{"id":"pwf9LGiCzYmo"}},{"cell_type":"code","source":["# Loss to be added in the custom loss\n","def get_chatgpt_rating(prompt, sample):\n","  completion = openai.ChatCompletion.create(\n","  model=\"gpt-3.5-turbo\",\n","  messages=[\n","    {\"role\": \"user\", \n","     \"content\": f\"{prompt} {sample}\"}\n","  ])\n","  return 10 - int(completion.choices[0].message)"],"metadata":{"id":"8tB5L6nFLhDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Different prompts tested in order to generate a sensible rating \n","prompt1 = \"Provide me rating between 0 and 10 (without any explanation), where 0 is the best and 10 is the worst, for the following story summary: \" \n","prompt2 = \"Assign a rating between 0 (best) and 10 (worst) to the given artificial story summary (only give rating as the response):\"\n","prompt3 = \"Assign a rating between 0 (best) and 10 (worst) to the given artificial story summary (only give rating as the response). The rating should be based on writing style, coherence and capture strength. Summary:\"\n","prompt4 = \"Assign a rating between 0 and 10 to the given artificial story summary. Only give rating as the response (no reasoning). The rating should be based on writing style, coherence, and capture strength. Summary:\" # Best\n","prompt5 = \"Provide me rating between 0 and 10 (without any explanation),  for the following story summary:\" "],"metadata":{"id":"MQJRNPghzb7g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Tokenization/Encoding"],"metadata":{"id":"2dYlSUZNQYop"}},{"cell_type":"code","source":["#Loading the standard T5 small model and tokenizer \n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","# Can try different T5 models such as T5-large, T5-3B, T5-11B\n","base_tokenizer = AutoTokenizer.from_pretrained('t5-base')\n","base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"],"metadata":{"id":"bVkVGdHBYQat"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoding the sequences\n","def encode_sequences(x, base_tokenizer = base_tokenizer):\n","  try:\n","    # Input consists of different aspects of the story on which the output will be conditioned\n","    input = str(x['Input'])\n","    # Label is the conditioned output - Story\n","    label = str(x['Summary'])\n","    # Max length of the input sequence in T5 is 512 tokens (BART could be used for longer sequences - 1024 max length limit) \n","    model_input = base_tokenizer(input, max_length = 512, truncation=True, padding='max_length')\n","    model_input['labels'] = base_tokenizer(label, max_length = 512, truncation=True, padding='max_length')['input_ids']\n","    return model_input\n","  except:\n","    # By performing this model will also be robust to empty inputs (a type of adversarial input)\n","    input = ''\n","    label = ''\n","    model_input = base_tokenizer(input, max_length = 512, truncation=True, padding='max_length')\n","    model_input['labels'] = base_tokenizer(label, max_length = 512, truncation=True, padding='max_length')['input_ids']\n","    return model_input"],"metadata":{"id":"pz4X3H2rMDTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the final dataset\n","import pandas as pd\n","df = pd.read_csv('')"],"metadata":{"id":"9F_s4EA1YTER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train_df, test_df = train_test_split(df, test_size=0.1)"],"metadata":{"id":"q7Y0PUMjTqO_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenizing the dataset\n","train_df = train_df.apply(encode_sequences(lambda x: x[['Input','Summary']]))\n","test_df = test_df.apply(encode_sequences(lambda x: x[['Input','Summary']]))"],"metadata":{"id":"DTsB1rXyMp-x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## T5 Training Setup (PyTorch) - Custom Loss Function"],"metadata":{"id":"a6xkj-aJUf6T"}},{"cell_type":"code","source":["# Edit code\n","import torch\n","\n","class IMDbDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = IMDbDataset(train_encodings, train_labels)\n","val_dataset = IMDbDataset(val_encodings, val_labels)\n","test_dataset = IMDbDataset(test_encodings, test_labels)"],"metadata":{"id":"2OhooFa5UgRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Edit code\n","from torch.utils.data import DataLoader\n","from transformers import DistilBertForSequenceClassification, AdamW\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","model.to(device)\n","model.train()\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","optim = AdamW(model.parameters(), lr=5e-5)\n","\n","for epoch in range(3):\n","    for batch in train_loader:\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optim.step()\n","\n","model.eval()"],"metadata":{"id":"RACWwTU6UoUq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## T5 Training Setup (Trainer) - Custom Loss Function"],"metadata":{"id":"_ASa0fbnQe4u"}},{"cell_type":"code","source":["# Initializing the Data Collator for batching of the dataset\n","from transformers import DataCollatorForSeq2Seq\n","data_collator = DataCollatorForSeq2Seq(\n","        tokenizer=base_tokenizer,\n","        return_tensors=\"pt\"\n","    )"],"metadata":{"id":"k_oMtcLHNMyo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","# Path where model training loss and intermediate weights will be stored\n","model_path = f'/content/drive/MyDrive/Visual Story Telling/Story_Gen_Model'\n","\n","#Specifying the training argument \n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=model_path,\n","    per_device_train_batch_size=4, \n","    overwrite_output_dir = True, \n","    evaluation_strategy=\"no\", \n","    gradient_accumulation_steps=8, \n","    num_train_epochs=15,\n","    weight_decay=0.01, \n","    lr_scheduler_type=\"cosine\",\n","    learning_rate=5e-4, \n","    fp16=True \n",")"],"metadata":{"id":"95wIk1YxNOTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initializing the trainer\n","trainer = Seq2SeqTrainer(\n","    model=base_model,                         # the instantiated  Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    data_collator=data_collator,\n","    train_dataset=train_df,       # training dataset\n","    eval_dataset = test_df\n",")"],"metadata":{"id":"DQsfkoziNn2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Overwrite the Trainer API for utilizing custom loss function\n","# Edit Trainer Functionality\n","class MyTrainer(Trainer):\n","  def __init__(self):\n","\n","  def compute_loss(self, model, inputs):\n","      labels = inputs.pop(\"labels\")\n","      outputs = model(**inputs)\n","      logits = outputs[0]\n","      return my_custom_loss(logits, labels)"],"metadata":{"id":"iSFkn4YDOXl1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training "],"metadata":{"id":"gpPf0iO7QiYn"}},{"cell_type":"code","source":["# Starting the training\n","trainer.train()"],"metadata":{"id":"RXKGeFzINqA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving the final model\n","trainer.save_model()"],"metadata":{"id":"u4ugp2eQNs_u"},"execution_count":null,"outputs":[]}]}